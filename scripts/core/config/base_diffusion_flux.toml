## utak atik

no_metadata = true
ae = "/app/flux/ae.safetensors"                                                                                                                                      
apply_t5_attn_mask = true                                                                                                                                                      
bucket_no_upscale = true                                                                                                                                                       
bucket_reso_steps = 64                                                                                                                                                         
cache_latents = true                                                                                                                                                           
cache_latents_to_disk = true                                                                                                                                                   
caption_extension = ".txt"                                                                                                                                                     
clip_l = "/app/flux/clip_l.safetensors"                                                                                                                              
discrete_flow_shift = 3.1582                                                                                                                                                   
dynamo_backend = "no"                                                                                                                                                          
# epoch = 100                                                                                                                                                                    
full_bf16 = true                                                                                                                                                               
gradient_accumulation_steps = 4                                                                                                                                                
gradient_checkpointing = true                                                                                                                                                  
guidance_scale = 1.0                                                                                                                                                     
highvram = true                                                                                                                                                                
huber_c = 0.1                                                                                                                                                                  
huber_scale = 1                                                                                                                                                                
huber_schedule = "snr"
huggingface_path_in_repo = "checkpoint"
huggingface_repo_id = ""
huggingface_repo_type = "model"
huggingface_repo_visibility = "public"
huggingface_token = ""                                                                                                                                                         
loss_type = "l2"                                                                                                                                                               
lr_scheduler = "cosine_with_min_lr" 
lr_warmup_steps = 10                                                                                                                                                     
lr_scheduler_args = []                                                                                                                                                         
lr_scheduler_num_cycles = 1                                                                                                                                                    
lr_scheduler_power = 1
lr_scheduler_min_lr_ratio = 0.25                                                                                                                                                         
max_bucket_reso = 2048                                                                                                                                                         
max_data_loader_n_workers = 2                                                                                                                                                  
max_timestep = 1000                                                                                                                                                            
max_train_epochs = 2                                                                                                                                                       
mem_eff_save = true                                                                                                                                                            
min_bucket_reso = 256                                                                                                                                                          
mixed_precision = "bf16"                                                                                                                                                       
model_prediction_type = "raw"                                                                                                                                                  
network_alpha = 64                                                                                                                                                            
network_args = [ "train_double_block_indices=all", "train_single_block_indices=all", "train_t5xxl=True",]                                                                      
network_dim = 128                                                                                                                                                              
network_module = "networks.lora_flux"                                                                                                                                          
# noise_offset_type = "Original"       
# noise_offset = 0.1                                                                                                                                          
optimizer_args = [ "betas=(0.9, 0.999)", "weight_decay=0.01", "eps=1e-08",]                                                              
optimizer_type = "adamw8bit"                                                                                                                                                   
output_dir = "/app/outputs"                                                                                                                                          
output_name = "last"                                                                                                                                                 
pretrained_model_name_or_path = "/app/flux/unet.safetensors"                                                                                                                  
prior_loss_weight = 1                                                                                                                                                          
resolution = "1024,1024"                                                                                                                                                       
sample_prompts = ""                                                                                                                    
sample_sampler = "euler_a"                                                                                                                                                     
save_every_n_epochs = 10                                                                                                                                                       
save_model_as = "safetensors"                                                                                                                                                  
save_precision = "float"                                                                                                                                                       
seed = 42                                                                                                                                                                       
t5xxl = "/app/flux/t5xxl_fp16.safetensors"                                                                                                                           
t5xxl_max_token_length = 512                                                                                                                                                   
text_encoder_lr = [ 0.0012035000000000001, 0.0012035000000000001,]                                                                                                                                               
timestep_sampling = "shift"                                                                                                                                                  
train_batch_size = 4                                                                                                                                                           
train_data_dir = ""                                                                                                                               
unet_lr = 0.0024070000000000003                                                                                                                                                
vae_batch_size = 4                                                                                                                                                             
wandb_run_name = "last"                                                                                                                                              
xformers = true 
clip_skip = 2     
min_snr_gamma = 5   